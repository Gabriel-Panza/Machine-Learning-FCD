{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste com Dataset Animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa todas as bibliotecas\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers, models, callbacks, metrics, Input, Model, regularizers\n",
    "import scipy.ndimage as ndi\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, auc, precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_animal_data(folder):\n",
    "    \"\"\"\n",
    "    Carrega os dados de um animal do diretório fornecido\n",
    "\n",
    "    Args:\n",
    "        folder (str): Caminho da pasta contendo os dados dos animais.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dados do paciente, incluindo imagens, máscaras e labels para os lados esquerdo e direito.\n",
    "              Retorna None se o paciente não for encontrado.\n",
    "    \"\"\"\n",
    "\n",
    "    types_image = [\"Dog\", \"Cat\"]\n",
    "    dogs = os.path.join(folder, types_image[0])\n",
    "    cats = os.path.join(folder, types_image[1])\n",
    "\n",
    "    # Verifica se os diretórios existem\n",
    "    if not os.path.exists(dogs) or not os.path.exists(cats):\n",
    "        print(f\"Estrutura de diretórios inválida para o animal fornecido.\")\n",
    "        return None\n",
    "\n",
    "    # Listas temporárias para armazenar os dados originais\n",
    "    dog_images = []\n",
    "    cat_images = []\n",
    "    dogs_label = [] \n",
    "    cats_label = []\n",
    "    \n",
    "    # Tamanho fixo para redimensionamento\n",
    "    target_size = (224, 224)\n",
    "\n",
    "    # Carrega as imagens e máscaras do lado esquerdo e direito\n",
    "    for dog in sorted(os.listdir(dogs)):\n",
    "        # Carrega os dados do cachorro\n",
    "        data_dog = cv2.imread(os.path.join(dogs, dog))\n",
    "        if data_dog is not None:\n",
    "            data_dog = cv2.resize(data_dog, target_size)\n",
    "            dog_images.append(data_dog)\n",
    "            dogs_label.append(1)\n",
    "    \n",
    "    for cat in sorted(os.listdir(cats)):\n",
    "        # Carrega os dados do gato\n",
    "        data_cat = cv2.imread(os.path.join(cats, cat))\n",
    "        if data_cat is not None:\n",
    "            data_cat = cv2.resize(data_cat, target_size)\n",
    "            cat_images.append(data_cat)\n",
    "            cats_label.append(0)\n",
    "            \n",
    "    print(f\"Total de recortes: {len(dog_images) + len(cat_images)}\")\n",
    "    \n",
    "    return dog_images, cat_images, dogs_label, cats_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmenter(input_shape, seed=42):\n",
    "    \"\"\"Cria uma camada de aumento de dados que aplica transformações aleatórias.\"\"\"\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\", seed=seed),\n",
    "        layers.RandomRotation(0.25, seed=seed),\n",
    "    ], name=\"augmenter\")\n",
    "\n",
    "def supervised_nt_xent_loss(projections, labels, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Calcula a perda NT-Xent supervisionada para classificação entre cães e gatos.\n",
    "    Considera pares da mesma classe como positivos, mesmo que sejam imagens diferentes.\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(projections)[0]\n",
    "    \n",
    "    # Normalizar as projeções\n",
    "    projections = tf.math.l2_normalize(projections, axis=1)\n",
    "    \n",
    "    # Calcular matriz de similaridade\n",
    "    similarity_matrix = tf.matmul(projections, projections, transpose_b=True)\n",
    "    \n",
    "    # Garantir que os labels tenham o formato correto\n",
    "    labels = tf.reshape(labels, [-1])\n",
    "    \n",
    "    # Criar máscara para pares positivos (imagens da mesma classe)\n",
    "    labels_matrix = tf.equal(labels[:, tf.newaxis], labels[tf.newaxis, :])\n",
    "    positive_mask = tf.cast(labels_matrix, tf.float32)\n",
    "    \n",
    "    # Remover a diagonal (não comparar uma imagem com ela mesma)\n",
    "    positive_mask = tf.linalg.set_diag(positive_mask, tf.zeros(batch_size))\n",
    "    \n",
    "    # Calcular a perda NT-Xent\n",
    "    logits = similarity_matrix / temperature\n",
    "    \n",
    "    # Para estabilidade numérica, subtrair o máximo dos logits\n",
    "    logits_max = tf.reduce_max(logits, axis=1, keepdims=True)\n",
    "    logits = logits - logits_max\n",
    "    \n",
    "    # Calcular a loss para cada exemplo\n",
    "    exp_logits = tf.exp(logits)\n",
    "    \n",
    "    # Máscara para pares negativos (imagens de classes diferentes)\n",
    "    negative_mask = 1 - positive_mask\n",
    "    negative_mask = tf.linalg.set_diag(negative_mask, tf.zeros(batch_size))\n",
    "    \n",
    "    # Calcular o denominador (soma sobre todos os pares negativos)\n",
    "    denominator = tf.reduce_sum(exp_logits * negative_mask, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calcular o numerador (soma sobre todos os pares positivos)\n",
    "    numerator = tf.reduce_sum(exp_logits * positive_mask, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calcular a loss para cada par positivo\n",
    "    loss = -tf.math.log(numerator / (numerator + denominator + 1e-8))\n",
    "    \n",
    "    # Calcular a média apenas sobre os pares positivos existentes\n",
    "    num_positive_pairs = tf.reduce_sum(positive_mask)\n",
    "    total_loss = tf.reduce_sum(loss * positive_mask) / (num_positive_pairs + 1e-8)\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "class SupervisedSimCLRModel(Model):\n",
    "    def __init__(self, encoder, projection_head, augmenter):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.projection_head = projection_head\n",
    "        self.augmenter = augmenter\n",
    "    \n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Recebemos imagens e labels\n",
    "        images, labels = data\n",
    "\n",
    "        # Gerar duas visões aumentadas de cada imagem\n",
    "        augmented_1 = self.augmenter(images)\n",
    "        augmented_2 = self.augmenter(images)\n",
    "        \n",
    "        # Concatenar todas as visões\n",
    "        all_views = tf.concat([augmented_1, augmented_2], axis=0)\n",
    "        \n",
    "        # Replicar os labels para corresponder às visões aumentadas\n",
    "        extended_labels = tf.concat([labels, labels], axis=0)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            embeddings = self.encoder(all_views)\n",
    "            projections = self.projection_head(embeddings)\n",
    "            \n",
    "            # Calcular a perda usando todas as projeções e labels estendidos\n",
    "            loss = supervised_nt_xent_loss(projections, extended_labels)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        \n",
    "        # Gerar duas visões aumentadas de cada imagem\n",
    "        augmented_1 = self.augmenter(images)\n",
    "        augmented_2 = self.augmenter(images)\n",
    "        \n",
    "        # Concatenar todas as visões\n",
    "        all_views = tf.concat([augmented_1, augmented_2], axis=0)\n",
    "        \n",
    "        # Replicar os labels para corresponder às visões aumentadas\n",
    "        extended_labels = tf.concat([labels, labels], axis=0)\n",
    "        \n",
    "        embeddings = self.encoder(all_views)\n",
    "        projections = self.projection_head(embeddings)\n",
    "        \n",
    "        loss = supervised_nt_xent_loss(projections, extended_labels)\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "def build_encoder(input_shape):\n",
    "    \"\"\"Constrói um encoder similar ao usado no SimCLR\"\"\"\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "    ], name=\"encoder\")\n",
    "\n",
    "def build_projection_head(embedding_dim=256, projection_dim=128):\n",
    "    \"\"\"Constrói uma cabeça de projeção\"\"\"\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=(embedding_dim,)),\n",
    "        layers.Dense(projection_dim, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(projection_dim),  # Sem ativação na última camada\n",
    "    ], name=\"projection_head\")\n",
    "    \n",
    "def build_classifier(encoder, input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Constrói um classificador tradicional usando o encoder pré-treinado.\n",
    "    \"\"\"\n",
    "    # Congelar o encoder inicialmente para fine-tuning gradual\n",
    "    encoder.trainable = False\n",
    "        \n",
    "    # Adicionar camadas de classificação\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        encoder,\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Criar o modelo\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', metrics.Precision(name=\"precision\"), metrics.Recall(name=\"recall\")])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de plot do treinamento do modelo\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Graphic')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Graphic')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Função de plot da matriz de confusão\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"D:/Dataset-Dogs_Cats\"\n",
    "dogs_data, cats_data, dogs_label_data, cats_label_data = load_animal_data(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Iniciando Etapa 1: Pré-treinamento Auto-Supervisionado com Labels ---\")\n",
    "\n",
    "# Preparar os dados de pré-treinamento (incluindo labels)\n",
    "pretrain_data = np.concatenate([dogs_data[:1000], cats_data[:1000]], axis=0)\n",
    "pretrain_label = np.concatenate([dogs_label_data[:1000], cats_label_data[:1000]], axis=0)\n",
    "print(f\"Total de imagens para treinamento: {len(pretrain_data)}\")\n",
    "\n",
    "input_shape = pretrain_data[0].shape\n",
    "print(f\"Shape das imagens para treinamento: {input_shape}\")\n",
    "\n",
    "encoder = build_encoder(input_shape)\n",
    "projection_head = build_projection_head()\n",
    "augmenter = get_augmenter(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sscl_model = SupervisedSimCLRModel(encoder, projection_head, augmenter)\n",
    "sscl_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
    "\n",
    "indices = np.arange(len(pretrain_data))\n",
    "np.random.shuffle(indices)\n",
    "pretrain_data = pretrain_data[indices]\n",
    "pretrain_labels = pretrain_label[indices]\n",
    "\n",
    "# Treinar o modelo contrastivo\n",
    "history_sscl = sscl_model.fit(\n",
    "    pretrain_data,\n",
    "    pretrain_label, \n",
    "    batch_size=64,\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "# Salvar os pesos do encoder pré-treinado\n",
    "encoder.save_weights('animal_encoder_pretrained.weights.h5')\n",
    "print(\"Pesos do encoder pré-treinado salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Iniciando Etapa 2: Fine-tuning Supervisionado ---\")\n",
    "\n",
    "# Construir o classificador siamês usando o encoder pré-treinado\n",
    "animal_classifier = build_classifier(encoder, input_shape)\n",
    "\n",
    "animal_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar os dados de treinamento (incluindo labels)\n",
    "train_data = np.concatenate([dogs_data[1000:3000], cats_data[1000:3000]], axis=0)\n",
    "train_label = np.concatenate([dogs_label_data[1000:3000], cats_label_data[1000:3000]], axis=0)\n",
    "# Preparar os dados de validaçao (incluindo labels)\n",
    "valid_data = np.concatenate([dogs_data[3000:3750], cats_data[3000:3750]], axis=0)\n",
    "valid_label = np.concatenate([dogs_label_data[3000:3750], cats_label_data[3000:3750]], axis=0)\n",
    "# Preparar os dados de teste (incluindo labels)\n",
    "test_data = np.concatenate([dogs_data[3750:4100], cats_data[3750:4100]], axis=0)\n",
    "test_label = np.concatenate([dogs_label_data[3750:4100], cats_label_data[3750:4100]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    'animal_trained.weights.h5', \n",
    "    monitor='val_loss', \n",
    "    save_best_only=True,  \n",
    "    save_weights_only=True, \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Treinamento do classificador\n",
    "history = animal_classifier.fit(\n",
    "    train_data,\n",
    "    train_label,\n",
    "    validation_data=(valid_data, valid_label),\n",
    "    batch_size=128, \n",
    "    epochs=50, \n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "print(\"Treinamento concluído com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar o histórico do treinamento\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando pesos da melhor época\n",
    "animal_classifier.load_weights('animal_trained.weights.h5')\n",
    "\n",
    "# Avaliar o modelo na validação\n",
    "y_pred_train = (animal_classifier.predict(train_data) > 0.5).astype(int)\n",
    "\n",
    "# Avaliar o modelo na validação\n",
    "y_pred_valid = (animal_classifier.predict(valid_data) > 0.5).astype(int)\n",
    "\n",
    "# Avaliar o modelo no teste\n",
    "y_pred_test = (animal_classifier.predict(test_data) > 0.5).astype(int)\n",
    "\n",
    "# Calcula a curva precision-recall\n",
    "precision, recall, _ = precision_recall_curve(test_label, y_pred_test)\n",
    "\n",
    "# Calcula a AUC precision-recall\n",
    "auc_pr = auc(recall, precision)\n",
    "\n",
    "# Plote a curva precision-recall\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'AUC = {auc_pr:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar o relatório de classificação\n",
    "print(\"Treino:\")\n",
    "print(classification_report(train_label, y_pred_train))\n",
    "print(\"\\n#########################################################\\n\")\n",
    "print(\"Validação:\")\n",
    "print(classification_report(valid_label, y_pred_valid))\n",
    "print(\"\\n#########################################################\\n\")\n",
    "print(\"Teste:\")\n",
    "#print(classification_report(y_test, y_pred_test)) \n",
    "print(classification_report(test_label, y_pred_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar a matriz de confusão\n",
    "print(\"Validação:\")\n",
    "plot_confusion_matrix(valid_label, y_pred_valid)\n",
    "print(\"Teste:\")\n",
    "plot_confusion_matrix(test_label, y_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
